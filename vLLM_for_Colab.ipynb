{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO+eNOrHttvmywfYbf1F21H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LossHunter/AI/blob/main/vLLM_for_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U vllm flashinfer-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xoa-iktj13Zx",
        "outputId": "4397c8cf-0c89-4a84-86e6-f1a75709670b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vllm in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.121.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.11.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.11.3)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.7.30)\n",
            "Requirement already satisfied: outlines_core==0.2.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.11)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (5.6.3)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.25 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.1.25)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1.1.post6)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.17.1)\n",
            "Requirement already satisfied: mistral_common>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
            "Requirement already satisfied: setuptools<80,>=77.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (79.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.11.0)\n",
            "Requirement already satisfied: depyf==0.19.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.2)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm) (1.1.1)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.16.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\n",
            "Requirement already satisfied: pybase64 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.4.2)\n",
            "Requirement already satisfied: cbor2 in /usr/local/lib/python3.12/dist-packages (from vllm) (5.7.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from vllm) (1.3.7)\n",
            "Requirement already satisfied: openai-harmony>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.0.8)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.61.2)\n",
            "Requirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.51.1)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.0+cu126)\n",
            "Requirement already satisfied: xformers==0.0.32.post1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.0.32.post1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.49.3)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.3)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.14)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.1)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.4.2)\n",
            "Requirement already satisfied: click!=8.3.0,>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\n",
            "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\n",
            "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.28.0)\n",
            "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.0.0)\n",
            "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.43.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqEviJca2e42",
        "outputId": "2e7d9300-418c-44ca-9765-da83c503d624"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!sudo tar xvzf ./ngrok-v3-stable-linux-amd64.tgz -C /usr/local/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ1bJjRa19MU",
        "outputId": "e0c3765d-cfdf-40f9-9275-3931d8aa21d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-07 01:23:18--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 13.248.244.96, 35.71.179.82, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9322550 (8.9M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   8.89M  3.82MB/s    in 2.3s    \n",
            "\n",
            "2025-11-07 01:23:21 (3.82 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz’ saved [9322550/9322550]\n",
            "\n",
            "ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NGROK_TOKEN = userdata.get(\"ngrok\")\n",
        "!ngrok config add-authtoken \"{NGROK_TOKEN}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c162jFpT2BNx",
        "outputId": "f0e1d622-959b-4fe7-fd21-24916ca7328d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ngrok http 8000 > ngrok.log 2>&1 &"
      ],
      "metadata": {
        "id": "Cpf_LuzU2GEO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model : DeepSeek R1 Distll Qwen 14B\n",
        "# !vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
        "\n",
        "# Load and run the model: Qwen3 30B -FP8\n",
        "# !vllm serve \"Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\"\n",
        "\n",
        "# Load and run the model: GPT-OSS 120B\n",
        "# 120B의 경우 GPU memory Util을 수정하여 실행해야함. (OOM 발생)\n",
        "!vllm serve openai/gpt-oss-120b \\\n",
        "  --dtype bfloat16 \\\n",
        "  --quantization mxfp4 \\\n",
        "  --max-model-len 4096 \\\n",
        "  --gpu-memory-utilization 0.92\n",
        "\n",
        "\n",
        "# Load and run the model: gemma3 27B\n",
        "# !vllm serve \"google/gemma-3-27b-it\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDTwyl6Q2Obw",
        "outputId": "f51aa1dc-a680-4af6-99e6-3f0c39e82aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-07 02:10:08 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:14 [api_server.py:1839] vLLM API server version 0.11.0\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:14 [utils.py:233] non-default args: {'model_tag': 'openai/gpt-oss-120b', 'model': 'openai/gpt-oss-120b', 'dtype': 'bfloat16', 'max_model_len': 4096, 'quantization': 'mxfp4', 'gpu_memory_utilization': 0.92}\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:16 [model.py:547] Resolved architecture: GptOssForCausalLM\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Parse safetensors files: 100% 15/15 [00:00<00:00, 28.68it/s]\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:17 [model.py:1510] Using max model len 4096\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:19 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:10:19 [config.py:271] Overriding max cuda graph capture size to 992 for performance.\n",
            "INFO 11-07 02:10:25 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:31 [core.py:644] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:31 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='openai/gpt-oss-120b', speculative_config=None, tokenizer='openai/gpt-oss-120b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=openai/gpt-oss-120b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[992,976,960,944,928,912,896,880,864,848,832,816,800,784,768,752,736,720,704,688,672,656,640,624,608,592,576,560,544,528,512,496,480,464,448,432,416,400,384,368,352,336,320,304,288,272,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":992,\"local_cache_dir\":null}\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:32 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m WARNING 11-07 02:10:32 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:32 [gpu_model_runner.py:2602] Starting to load model openai/gpt-oss-120b...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:33 [gpu_model_runner.py:2634] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:33 [cuda.py:361] Using Triton backend on V1 engine.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:33 [mxfp4.py:98] Using Marlin backend\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:33 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 15/15 [00:19<00:00,  1.32s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:54 [default_loader.py:267] Loading weights took 19.92 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m WARNING 11-07 02:10:54 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:10:58 [gpu_model_runner.py:2653] Model loading took 65.9651 GiB and 24.686661 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:06 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/00af691bd2/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:06 [backends.py:559] Dynamo bytecode transform time: 7.62 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:10 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.757 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:11 [marlin_utils.py:353] You are running Marlin kernel with bf16 on GPUs before SM90. You can consider change to fp16 to achieve better performance if possible.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:11 [monitor.py:34] torch.compile takes 7.62 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:12 [gpu_worker.py:298] Available KV cache memory: 5.13 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:12 [kv_cache_utils.py:1087] GPU KV cache size: 74,640 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:12 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 23.74x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 81/81 [00:08<00:00,  9.10it/s]\n",
            "Capturing CUDA graphs (decode, FULL): 100% 35/35 [00:04<00:00,  8.51it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:26 [gpu_model_runner.py:3480] Graph capturing finished in 14 secs, took 1.30 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=14958)\u001b[0;0m INFO 11-07 02:11:26 [core.py:210] init engine (profile, create kv cache, warmup model) took 28.34 seconds\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:29 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 9331\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:29 [api_server.py:1634] Supported_tasks: ['generate']\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m WARNING 11-07 02:11:29 [serving_responses.py:154] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:34] Available routes are:\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /health, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /load, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /ping, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /ping, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /tokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /detokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/models, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /version, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /pooling, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /classify, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /invocations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:11:34 [launcher.py:42] Route: /metrics, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m14821\u001b[0m]\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m \u001b[32mINFO\u001b[0m:     183.91.192.218:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:12:34 [loggers.py:127] Engine 000: Avg prompt throughput: 7.5 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:12:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:24:24 [loggers.py:127] Engine 000: Avg prompt throughput: 7.3 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 43.2%\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m \u001b[32mINFO\u001b[0m:     183.91.192.218:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:24:34 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.2%\n",
            "\u001b[1;36m(APIServer pid=14821)\u001b[0;0m INFO 11-07 02:24:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ss -lntp | grep -E \":8000\"\n"
      ],
      "metadata": {
        "id": "x8GQneCN4vBO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kjbdClK3NgS",
        "outputId": "6730024f-040d-40fd-f0a1-7a6fbb5f18ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line\",\"ID\":\"77d4a36e91b78c1719f92a84f03bf43b\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://unmummied-keshia-feelingly.ngrok-free.dev\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:8000\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    }
  ]
}